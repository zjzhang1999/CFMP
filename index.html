<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="A Four-Modality Fusion Method Of Crowdfunding Prediction">
  <meta name="keywords" content="Vision-Language Grounding, Manipulation, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CFMP</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="./static/js/gtag.js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-4Y34PZ3XBE');

  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("single-task-result-video");
      video.src = `./media/web-results/${demo}-dr${task}-${inst}.mp4`
      
      video.playbackRate = 2.0;
      video.play();
    }

    function updateMultiVideo() {
      var demo = document.getElementById("multi-menu-demos").value;
      var task = document.getElementById("multi-menu-tasks").value;
      var inst = document.getElementById("multi-menu-instances").value;

      console.log("multi", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "https://cliport.github.io/media/results_web/" +
        demo +
        task +
        inst +
        ".mp4";
      video.playbackRate = 2.0;
      video.play();
    }

  </script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="./static/js/jq.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" target="_blank" href="https://mohitshridhar.com">
          <span class="icon">
            <!-- <i class="fas fa-home"></i> -->
          </span>
        </a>

        <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://askforalfred.com/">
            ALFRED
          </a>
          <a class="navbar-item" target="_blank" href="http://alfworld.github.io/">
            ALFWorld
          </a>
          <a class="navbar-item" target="_blank" href="https://arxiv.org/pdf/1806.03831.pdf">
            INGRESS
          </a>
        </div>
      </div> -->
      </div>

    </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">A Four-Modality Fusion Method Of Crowdfunding Prediction</h1>
            <!-- <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.robot-learning.org/">CoRL 2021</a></h3> -->
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank" href="https://zjzhang1999.github.io">Zijian ZHANG</a></span>
              <!-- <span class="author-block">
              <a target="_blank" href="http://lucasmanuelli.com/">Lucas Manuelli</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a><sup>1, 2</sup>
            </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup>City University of Hong Kong </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="./media/paper/A Four-Modality Fusion Method For CF-prediction.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!--               <span class="link-block">
                <a target="_blank" href="https://drive.google.com/file/d/1xzG5e1XF958HPuD_FZTiKROd9AQyd1fS/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
                <!-- Video Link. -->

                  <!-- <a target="_blank" href="https://youtu.be/UdzoagBgWTA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a> -->
                 
                <!--Data Link-->
                <span class="link-block">
                  <a href="https://www.kickstarter.com/discover/advanced?sort=magic&seed=2833567&next_page_cursor=&page=1"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                    </a>
                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/zjzhang1999/cfmp_new.github.io"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-fullhd">
      <div class="hero-body">
        <img src="./static/images/bg.png" alt="">
        <!-- <video id="teaser" autoplay muted loop height="100%">
        <source src="https://cliport.github.io/media/videos/10sim_web_teaser.mp4"
                type="video/mp4">
      </video> -->
        <h2 class="subtitle has-text-centered">
          </br>
          <span class="dcliport">The</span> above is my <b>Four-Modality</b> Fusion Method of crowdfunding prediction as well as the crowdfunding dataset <b>Kick60K</b> we collected.
        </h2>
      </div>
    </div>
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop height="100%">
              
              <source src="./media/videos/sigmoid-dr0-1e-4.mp4" type="video/mp4">
              <!--source src="./media/videos/1005ffd7b942c362f271825e3cfdc080.mp4" type="video/mp4"-->
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop height="100%">
              <source src="./media/videos/33ccd8e595720bc29e22a8ee64dcd38d.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop height="100%">
              <source src="./media/videos/355ad7f4a009fd076d22e949fce55f07.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
              <source src="./media/videos/3ecbfeec74610b95b040ac56536153d3.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop height="100%">
              <source src="./media/videos/4e11ba707def1870e6cb2e76ce5dcf4d.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop height="100%">
              <source src="./media/videos/80e41061b3471566df0b3b9853d5172c.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop height="100%">
              <source src="./media/videos/a4e83b9b2de5ab70a28352a796b4d1d7.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
              <source src="./media/videos/7dd4c.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop height="100%">
              <source src="./media/videos/3ecbfeec74610b95b040ac56536153d3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
  <h2 class="subtitle has-text-centered">
    </br>
    <p>Here I present the <b>visual modality (video) </b>of 10 category of the crowdfunding projects</b> each of them have indivisual narration styles and visual cues. </p>Each video clips,
    <b>32</b> frames are selected.  Data Source: <a href="https://www.kickstarter.com/">Kickstarter (Our dataset Kick60K)</a>
  </h2>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Would you invest your money in an idea you don't actually see? Can my project be funded and realized? 
              This work may provide some shallow reference both for investors and creators .
            </p>
            <p>
              The goal of crowdfunding ex-ante prediction is to extract the most relevant information about the project in 
              the context of crowdfunding to form reasoning. Unlike ex-post prediction, the ex-ante prediction does not rely 
              on information obtained after the project is released. However, existing multimodality method fail to represent
              and align long sequences in the context of crowdfunding effectively, resulting in information noise and decreased 
              prediction performance. To address this issue, we introduce a four-modality fusion method, which can realize modular 
              extraction of multiple modalities to adapt to large-scale modality missing datasets and achieve semantic alignment 
              and fusion in video frame space.
            </p>
            <p>
              In addition, we propose two large-scale multimodality crowdfunding datasets, ,<b>Kick30</b> and <b>Kick60</b>, each containing <b>30,000</b>
              and <b>68,000</b> crowdfunding projects respectively. The former also includes video frame features extracted by TimeSformer 
              and textual sequence features extracted by BERT. The experiment on Kick30 demonstrate the superiority of our method, 
              achieving state-of-the-art performance in 10/15 of the category. Extensive modality ablation study on both dataset shows 
              the importance of visual and textual information in the reasoning process. 

            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

    </div>

    <!-- Paper video. -->
    </br>
    </br>
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/UdzoagBgWTA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->

  </section>

  <section class="section">
    <div class="container is-max-widescreen">

      <div class="rows">


        <!-- Animation. -->
        <div class="rows is-centered ">
          <div class="row is-full-width">
            <h2 class="title is-3"><span class="dcliport">Kick60</span></h2>

            <!-- Interpolating. -->
            <h3 class="title is-4">Dataset</h3>
            <div class="content has-text-justified">
              <p>
                We collected a multimodal crowdfunding dataset with 60k+ video-text-metadata pairs,Kick60K. Limited by computer storage space, 
                we only used half of the data for the experiment, which we called Kick30K. It covers 15 categories,172 sub-categories 
                and 2 statuses on Kickstarter, the most famous crowdfunding platform in the world. Each category with 1,000 positive and
                1,000 negative samples, respectively. Each sample contains a project video, text introduction, metadata, and a label 
                for success or failure. The dataset is split into training, validation, and test splits with 18,000, 6,000 and 6,000 
                samples, respectively.
                Besides, Kick60K will be used for extensive experiment to explore the weights of different modality in the module. 
                It contains around 68,000 such samples across 15 categories distributed all the word. 18,000 of them have at least 
                one missing modality. 
              </p>
            </div>
            <img src="./media/images/dataset.png" class="interpolation-image"
              alt="Interpolate start reference image." />
            <br />
            
            <br />
            <br />
            <br />
            <!--/ Interpolating. -->

            <!-- Re-rendering. -->
            <h3 class="title is-4">Data Statistics</h3>
            <div class="content has-text-justified">
              <p>
                Here we present more detailed statistics around our dataset. 
                Figure1 presents the video introduction time distribution (seconds) and Figure2 
                presents the video introduction frames distribution. Figure3 shows the distribution 
                of text words in story background. Figure4 illustrates the launch date(year) and Figure5 
                shows the duration of projects. Figure6 presents the average of video time and frames of all categories.

              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./media/images/dataset2.png"
                alt="">
              <!-- <video id="transporter-gif" controls muted autoplay loop width="40%">
                <source src="https://transporternets.github.io/images/animation.mp4" type="video/mp4">
              </video> -->
            </div>
            <br />
            <br />
            <br />
            <!--
            <h3 class="title is-4">TransporterNets2</h3>
            <div class="content has-text-justified">
              <p>
                We use this two-stream architecture in all three networks of <a target=”_blank”
                  href="https://transporternets.github.io/">TransporterNets</a>
                to predict pick and place affordances at each timestep. TransporterNets first attends to a local region
                to decide where to pick,
                then computes a placement location by finding the best match for the picked region through
                cross-correlation of deep visual features. This structure serves as a powerful inductive bias for
                learning <a target="_blank"
                  href="https://fabianfuchsml.github.io/equivariance1of2/">roto-translationally equivariant</a>
                representations in tabletop environments.

              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="https://landingpage-image-1304214004.file.myqcloud.com/cc20210827102234/0dbe5abe-1b4e-4a77-aa66-758e6d26a9cbPC%20banner.png?imageMogr2/format/webp"
                alt="">
              <p>
                Credit: <a href="https://transporternets.github.io/">Zeng et. al (Google)</a>
              </p>
            </div>
            <br />
            <b>Paradigm 2:</b> TransporterNets takes an <a target="_blank"
              href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> to perception where
            the objective is to <i>detect actions</i> rather than <i>detect objects</i> and then learn a policy. Keeping
            the action-space grounded in the perceptual input allows us to exploit geometric symmetries for efficient
            representation learning.
            When combined with CLIP's pre-trained representations, this enables the learning of reusable manipulation
            skills without any "objectness" assumptions.
            <br />
            <br />
            <br />
            -->
            <!--/ Re-rendering. -->

            <h2 class="title is-3">Methods</h2>

            <div class="columns">
              <div class="column has-text-centered">
                <h3 class="title is-5">Train Curve in Kick30K</h3>

                Trained with
                <div class="select is-small">
                  <select id="single-menu-demos" onchange="updateSingleVideo()">
                    <option value="relu">relu</option>
                    <option value="w/o ReLU">w/o ReLU</option>
                    <option value="Tanh">Tanh</option>
                    <option value="sigmoid" selected="selected">sigmoid</option>
                  </select>
                </div>
                activation, with dropout rate
                <div class="select is-small">
                  <select id="single-menu-tasks" onchange="updateSingleVideo()">
                    <option value="0" selected="selected">0</option>
                    <option value="0.3">0.3</option>
                    <option value="0.5">0.5</option>
                  </select>
                </div>
                learning rate
                <div class="select is-small">
                  <select id="single-menu-instances" onchange="updateSingleVideo()">
                    <option value="5e-4">5e-4</option>
                    <option value="1e-4" selected="selected">1e-4</option>
                    <option value="1e-3">1e-3</option>
                  </select>
                </div>
                
                
                <!--img class="single-task-result-video"
                  src="https://landingpage-image-1304214004.file.myqcloud.com/cc20210827102234/942f4183-605d-4353-9a55-c73b03c7e979%E5%BC%A7%E7%84%8A%E5%B7%A5%E4%BD%9C%E7%AB%99.jpg?imageMogr2/format/webp"
                  alt=""-->
                 <video id="single-task-result-video" controls muted autoplay loop width="100%">
                  <source
                    src="./media/videos/sigmoid-dr0-1e-4.mp4"
                    type="video/mp4">
               
                </video> 
              </div>

              
              <div class="column has-text-centered">
                <h3 class="title is-5">Train Curve in Kick60K</h3>

                Trained with
                <div class="select is-small">
                  <select id="single-menu-demos" onchange="updateSingleVideo()">
                    <option value="ReLU">ReLU</option>
                    <option value="w/o ReLU">w/o ReLU</option>
                    <option value="Tanh">Tanh</option>
                    <option value="sigmoid" selected="selected">sigmoid</option>
                  </select>
                </div>
                activation, with dropout rate
                <div class="select is-small">
                  <select id="single-menu-tasks" onchange="updateSingleVideo()">
                    <option value="0">0</option>
                    <option value="0.3">0.3</option>
                    <option value="0.5">0.5</option>
                  </select>
                </div>
                learning rate
                <div class="select is-small">
                  <select id="single-menu-instances" onchange="updateSingleVideo()">
                    <option value="5e-4">5e-4</option>
                    <option value="1e-4">1e-4</option>
                    <option value="1e-3">1e-3</option>
                  </select>
                </div>
                </br>
                </br>
                <!--img class="multi-task-result-video"
                  src="https://landingpage-image-1304214004.file.myqcloud.com/cc20210827102234/a8bb0eab-9832-4609-93e8-1b4a459ff129%E6%99%BA%E6%85%A7%E6%A3%80%E9%AA%8C%E7%AB%99.jpg?imageMogr2/format/webp"
                  alt=""-->
                  <video id="multi-task-result-video" controls muted autoplay loop width="100%"> 
                  <source src="./media/web-results/Tanh-0-5e-4.mp4"
                     type="video/mp4">
                  </video>

              </div>
            </div>
            
            </br>
            <h3 class="title is-4">Main results</h3>
            <div class="content has-text-justified">
              <p>
                We perform crowdfunding reasoning tasks with the Kick30 we independently collected. 
                Table1 shows the main results. Column game and theatre are the two results which test 
                on game and theatre category project only, and column arts and dance are the two results 
                which test on arts and dance category project only. The former two achieve top2 accuracy 
                of our method, while the latter are the bottom2.   

              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./media/images/mainresults1.png"
                alt="">
              <br />
              <p>
                Further details for all categories results can be seen in the Paper Appendix. </details>: <a href="./media/paper/A Four-Modality Fusion Method For CF-prediction.pdf">Zhang(2023)</a>
              </p>
              <br />
              <br />
              <br />
              <img
                src="./media/images/mainresults2.png"
                alt="">




            </div>
            <h3 class="title is-4">Ablation analysis</h3>
            <div class="content has-text-justified">
              <p>
                To further investigate the contribution of each component in our method, we conduct ablation studies for different feature backbone and different modalities. 
              
              </p>
            </div>
            <br />
            <img style="height: 400px;object-fit: cover;width: 100%;"
              src="./media/images/abalation.png"
              class="interpolation-image" alt="Interpolate start reference image." />
            <br />
            <br />

            <h3 class="title is-4">Visualization analysis</h3>
            <div class="content has-text-justified">
              <p>
                Figure 5 shows the modality visual comparison between video 
                frame and audio with top3 and bottom3 attention weight each.
                This project is about the design of antique plates. We observed 
                that high-weight video frames more clearly reflected product 
                visual details and craftsmen working scenes
              </p>
            </div>
            <br />
            <img style="height: 400px;object-fit: cover;width: 100%;"
              src="./media/images/vis.png"
              class="interpolation-image" alt="Interpolate start reference image." />
            <br />

          </div>
        </div>

      </div>
  </section>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <p>
              <!--Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by
              the amazing <a href="https://keunhong.com/">Keunhong Park</a>.-->
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
